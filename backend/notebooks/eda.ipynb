{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e702f77a",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c20574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "import cv2 \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchvision import transforms \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(2002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c11e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512a2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../public/data\"\n",
    "PATH_GOOD = os.path.join(PATH, \"good\")\n",
    "PATH_OIL = os.path.join(PATH, \"oil\")\n",
    "SCRATCH_PATH = os.path.join(PATH, \"scratch\")\n",
    "STAIN_PATH = os.path.join(PATH, \"stain\")\n",
    "\n",
    "GROUND_TRUTH_PATH1 = os.path.join(PATH, \"ground_truth_1\")\n",
    "GROUND_TRUTH_PATH2 = os.path.join(PATH, \"ground_truth_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5adc2",
   "metadata": {},
   "source": [
    "## 1.1 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc27a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def show_images(images, titles=None, cols=5, figsize=(15, 10)):\n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image)\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# show_images([read_image(os.path.join(PATH_GOOD, img)) for img in os.listdir(PATH_GOOD)[:10]], titles=os.listdir(PATH_GOOD)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1baf5e",
   "metadata": {},
   "source": [
    "# 2. Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4c3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = glob.glob(os.path.join(PATH, \"*\", \"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf6e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coco_annotations(root_dir, output_file):\n",
    "    \"\"\"\n",
    "    Quét qua thư mục dữ liệu và tạo file chú thích định dạng COCO.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Khởi tạo cấu trúc file COCO ---\n",
    "    info = {\n",
    "        \"description\": \"Phone Defect Dataset\",\n",
    "        \"url\": \"\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": datetime.date.today().year,\n",
    "        \"contributor\": \"Your Name\",\n",
    "        \"date_created\": datetime.date.today().isoformat()\n",
    "    }\n",
    "\n",
    "    licenses = [{\"url\": \"\", \"id\": 0, \"name\": \"License\"}]\n",
    "\n",
    "    # Định nghĩa các lớp lỗi. ID 0 thường dành cho background.\n",
    "    categories = [\n",
    "        {\"id\": 1, \"name\": \"scratch\", \"supercategory\": \"defect\"},\n",
    "        {\"id\": 2, \"name\": \"stain\", \"supercategory\": \"defect\"},\n",
    "        {\"id\": 3, \"name\": \"oil\", \"supercategory\": \"defect\"}\n",
    "    ]\n",
    "    \n",
    "    # Tạo category mapping để dễ tra cứu\n",
    "    category_map = {cat['name']: cat['id'] for cat in categories}\n",
    "\n",
    "    coco_output = {\n",
    "        \"info\": info,\n",
    "        \"licenses\": licenses,\n",
    "        \"categories\": categories,\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    image_id_counter = 1\n",
    "    annotation_id_counter = 1\n",
    "\n",
    "    # # --- 2. Xử lý các thư mục chứa lỗi ---\n",
    "    defect_folders = [\"scratch\", \"stain\", \"oil\"]\n",
    "    ground_truth_folders = [\"ground_truth_1\", \"ground_truth_2\"]\n",
    "\n",
    "    for category_name in defect_folders:\n",
    "        category_id = category_map[category_name]\n",
    "        image_folder = os.path.join(root_dir, category_name)\n",
    "        \n",
    "        if not os.path.isdir(image_folder):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing folder: {category_name}\")\n",
    "        for image_filename in tqdm(os.listdir(image_folder)):\n",
    "            image_path = os.path.join(image_folder, image_filename)\n",
    "            \n",
    "            # Đọc ảnh để lấy kích thước\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "            except IOError:\n",
    "                print(f\"Warning: Could not read image {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Thêm thông tin ảnh vào danh sách\n",
    "            image_info = {\n",
    "                \"id\": image_id_counter,\n",
    "                \"file_name\": os.path.join(category_name, image_filename),\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            }\n",
    "            coco_output[\"images\"].append(image_info)\n",
    "\n",
    "            # Tìm mask tương ứng\n",
    "            # Giả định tên mask có hậu tố '_mask.png'\n",
    "            mask_filename = os.path.splitext(image_filename)[0] + '.png' \n",
    "            mask_path = None\n",
    "            for gt_folder in ground_truth_folders:\n",
    "                potential_path = os.path.join(root_dir, gt_folder, mask_filename)\n",
    "                if os.path.exists(potential_path):\n",
    "                    mask_path = potential_path\n",
    "                    break\n",
    "            \n",
    "            if mask_path:\n",
    "                # Chuyển mask thành polygon cho COCO\n",
    "                mask_image = Image.open(mask_path).convert('L')\n",
    "                mask_np = np.array(mask_image)\n",
    "                \n",
    "                # Tìm các đường viền trong mask (mỗi đường là một vùng lỗi)\n",
    "                # 0.5 là ngưỡng để coi pixel là một phần của đối tượng\n",
    "                contours = measure.find_contours(mask_np, 0.5)\n",
    "\n",
    "                for contour in contours:\n",
    "                    # Chuyển contour thành list [x1, y1, x2, y2, ...]\n",
    "                    contour = np.flip(contour, axis=1) # Đảo (row, col) thành (x, y)\n",
    "                    segmentation = contour.ravel().tolist()\n",
    "\n",
    "                    # Chỉ thêm vào nếu polygon có ít nhất 3 điểm (6 tọa độ)\n",
    "                    if len(segmentation) < 6:\n",
    "                        continue\n",
    "\n",
    "                    # Tính bounding box [x, y, width, height]\n",
    "                    x_coords = contour[:, 0]\n",
    "                    y_coords = contour[:, 1]\n",
    "                    x_min, x_max = np.min(x_coords), np.max(x_coords)\n",
    "                    y_min, y_max = np.min(y_coords), np.max(y_coords)\n",
    "                    bbox = [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]\n",
    "\n",
    "                    # Tính diện tích\n",
    "                    area = (x_max - x_min) * (y_max - y_min)\n",
    "                    \n",
    "                    annotation_info = {\n",
    "                        \"id\": annotation_id_counter,\n",
    "                        \"image_id\": image_id_counter,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"segmentation\": [segmentation], # COCO format yêu cầu list của các list\n",
    "                        \"area\": float(area),\n",
    "                        \"bbox\": bbox,\n",
    "                        \"iscrowd\": 0\n",
    "                    }\n",
    "                    coco_output[\"annotations\"].append(annotation_info)\n",
    "                    annotation_id_counter += 1\n",
    "\n",
    "            image_id_counter += 1\n",
    "\n",
    "    # --- 3. Xử lý thư mục \"good\" (chỉ thêm thông tin ảnh, không có annotation) ---\n",
    "    good_folder = os.path.join(root_dir, \"good\")\n",
    "    if os.path.isdir(good_folder):\n",
    "        print(\"Processing folder: good\")\n",
    "        for image_filename in tqdm(os.listdir(good_folder)):\n",
    "            image_path = os.path.join(good_folder, image_filename)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "            except IOError:\n",
    "                print(f\"Warning: Could not read image {image_path}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            image_info = {\n",
    "                \"id\": image_id_counter,\n",
    "                \"file_name\": os.path.join(\"good\", image_filename),\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            }\n",
    "            coco_output[\"images\"].append(image_info)\n",
    "            image_id_counter += 1\n",
    "\n",
    "    # --- 4. Lưu file JSON ---\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_output, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nSuccessfully created COCO annotation file at: {output_file}\")\n",
    "\n",
    "dataset_root_directory = PATH\n",
    "    \n",
    "# Tên file JSON đầu ra\n",
    "output_json_file = os.path.join(dataset_root_directory, 'annotations.json')\n",
    "\n",
    "# create_coco_annotations(dataset_root_directory, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2727b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    train_transforms = A.Compose([\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=20, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return train_transforms\n",
    "\n",
    "def valid_transform():\n",
    "    valid_transforms = A.Compose([\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return valid_transforms\n",
    "\n",
    "def train_valid_split(X, test_size = 0.2):\n",
    "    test_size = int(len(X) * test_size)\n",
    "    X_train = X[:-test_size]\n",
    "    X_valid = X[-test_size:]\n",
    "    return X_train, X_valid \n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bb6c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img_Segmentation_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transforms=None, image_ids=None): # Thêm image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.coco = COCO(annotation_file)\n",
    "        \n",
    "        # Lọc ID nếu được cung cấp, nếu không thì lấy tất cả\n",
    "        if image_ids:\n",
    "            self.ids = image_ids\n",
    "        else:\n",
    "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        \n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        mask = np.zeros((img_info[\"height\"], img_info[\"width\"]), dtype=np.uint8)\n",
    "        \n",
    "        for ann in annotations:\n",
    "            category_id = ann[\"category_id\"]\n",
    "            single_ann_mask = self.coco.annToMask(ann)\n",
    "            mask[single_ann_mask == 1] = category_id\n",
    "            \n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float16)\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ee063b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(\"../../public/processed/annotations.json\")\n",
    "all_image_ids = list(sorted(coco.imgs.keys()))\n",
    "\n",
    "# Chia danh sách ID thành train và valid\n",
    "train_ids, valid_ids = train_valid_split(all_image_ids, test_size=0.2)\n",
    "\n",
    "# Khởi tạo Dataset với các tập ID tương ứng\n",
    "DATASET_ROOT = \"../../public/data/\" # Thư mục gốc chứa các folder ảnh\n",
    "ANNOTATION_FILE = \"../../public/processed/annotations.json\"\n",
    "\n",
    "train_dataset = Img_Segmentation_Dataset(DATASET_ROOT, ANNOTATION_FILE, transforms=train_transform(), image_ids=train_ids)\n",
    "valid_dataset = Img_Segmentation_Dataset(DATASET_ROOT, ANNOTATION_FILE, transforms=valid_transform(), image_ids=valid_ids)\n",
    "\n",
    "# DataLoader vẫn giữ nguyên\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6a2afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26295/2815842037.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image, dtype=torch.float16)\n",
      "/tmp/ipykernel_26295/2815842037.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "img, mask = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d41cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class conv_block(nn.Module):\n",
    "#     def __init__(self, in_c, out_c):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_c),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_c),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# class encoder(nn.Module):\n",
    "#     def __init__(self, in_c, out_c):\n",
    "#         super().__init__()\n",
    "#         self.conv = conv_block(in_c, out_c)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         p = self.pool(x)\n",
    "#         return x, p\n",
    "    \n",
    "# class decoder(nn.Module):\n",
    "#     def __init__(self, in_c, out_c):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size = 2, stride =2 , padding = 0)\n",
    "#         self.conv = conv_block(out_c + out_c , out_c)\n",
    "        \n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.up(x)\n",
    "#         x = torch.cat([x, skip], axis = 1)\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "    \n",
    "# class UNET(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.enc1 = encoder(3, 64)\n",
    "#         self.enc2 = encoder(64, 128)\n",
    "#         self.enc3 = encoder(128, 256)\n",
    "#         self.enc4 = encoder(256, 512)\n",
    "        \n",
    "#         self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "#         self.dec1 = decoder(1024, 512)  \n",
    "#         self.dec2 = decoder(512, 256)\n",
    "#         self.dec3 = decoder(256, 128)\n",
    "#         self.dec4 = decoder(128, 64)\n",
    "        \n",
    "#         self.outputs = nn.Conv2d(64, 4, kernel_size=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         s1, p1 = self.enc1(x)\n",
    "#         s2, p2 = self.enc2(p1)\n",
    "#         s3, p3 = self.enc3(p2)\n",
    "#         s4, p4 = self.enc4(p3)\n",
    "        \n",
    "#         b = self.bottleneck(p4)\n",
    "        \n",
    "#         d1 = self.dec1(b, s4)\n",
    "#         d2 = self.dec2(d1, s3)\n",
    "#         d3 = self.dec3(d2, s2)\n",
    "#         d4 = self.dec4(d3, s1)\n",
    "        \n",
    "#         outputs = self.outputs(d4)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2e06d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinhquy/Desktop/Code/AI/Detection-System/backend/myenv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dinhquy/Desktop/Code/AI/Detection-System/backend/myenv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.segmentation as models\n",
    "\n",
    "model = models.deeplabv3_resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "81710e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[4] = torch.nn.Conv2d(256, 4,  kernel_size=(1, 1), stride=(1, 1))\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "994cbe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNET().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9ebfbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in tqdm(train_data_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)[\"out\"]\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate(valid_data_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(valid_data_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(valid_data_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def run(train_data_loader, valid_data_loader, model, criterion, optimizer, device, num_epochs):\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss = train(train_data_loader, model, criterion, optimizer, device)\n",
    "        valid_loss = validate(valid_data_loader, model, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        # Lưu mô hình tốt nhất\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Best model saved.\")\n",
    "    \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3442b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/244 [00:00<?, ?it/s]/tmp/ipykernel_26295/2815842037.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image, dtype=torch.float16)\n",
      "/tmp/ipykernel_26295/2815842037.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask, dtype=torch.long)\n",
      "  0%|          | 1/244 [10:08<41:05:05, 608.67s/it]"
     ]
    }
   ],
   "source": [
    "run(train_data_loader, valid_data_loader, model, criterion, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc62132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
